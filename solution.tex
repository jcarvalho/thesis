\chapter{Solution}
\label{chap:solution}

With a proper understanding of the Fenix Framework, this chapter
describes the solution proposed to solve the problem of relieving
programmers of the effort of programming Long Lived
Transactions. Section~\ref{sec:challenges} describes the challenges
faced when implementing Long Lived
Transactions. Section~\ref{sec:arch} describes the architecture of the
proposed solution, with the rationales for each design
decision. Section~\ref{sec:impl} describes how the proposed
architecture was implemented on top of the Fenix Framework using the
JVSTM. Finally, Section~\ref{sec:validation} shows that both the
architecture and implementation fullfil all the requirements, and
attempts to measure the effort required to use the implementation.

\section{Challenges}
\label{sec:challenges}

Due to the nature of Software Transactional Memories, there are a few
challenges when implementing Long Lived Transactions:

\begin{itemize}

\item How is information carried between the various steps of the
  transaction?

\item How to use a unique representation for every possible value any
  domain slot can take?

\item How to ensure that multiple users can concurrently access a Long
  Lived Transaction?

\end{itemize}

\section{Architecture}
\label{sec:arch}

The main goal of this solution is to relieve programmers of the burden
of dealing with Long Lived Transactions, making the effort needed to
program one similar to the effort of programming a regular
transaction.

So, what does the single interaction scenario has that makes it so
easy to program? It has a single transactional context that spans the
whole operation (provided by a regular transaction, as that they have
the same lifespan). In the multiple interaction scenario the system
transaction was shorter than the business transaction, so in each step
the context was lost.

Looking at the information that is kept during the lifespan of a
regular transaction, we can identify the three major pieces:

\begin{itemize}
\item The version in which the transaction is running. This version
  number corresponds to the logical point in time in which the
  transaction occurred (i.e. its serialization point).
\item A list of all the items written throughout the transaction (and
  the respective written values). This is the critical piece, as it
  contains the updated data that will be written to the global context
  on transaction's commit.
\item A list of all elements read throughout the transaction. This
  piece of information is critical to ensure the correctness of the
  operation, as the outcome of the transaction depends on the values
  of all the read data.
\end{itemize}

These pieces are crucial to ensure the correct operation of an
STM-based transactional system. STM libraries provide them for regular
(short-lived) transactions. The solution presented below aims to
provide them for Long Lived Transactions, using the short-lived
transactions as its building blocks.

Short lived transactions keep all the necessary information in
transient transaction-local storage (typically in memory), until the
time they commit, merging the write set with the global context. With
Long Lived Transactions, merging this write set in each step is not
possible. 

Recall the Course creation example. Consider that in the first step a
Course is created with only a name and the department it is associated
to. Merging the Write Set for this step with the global context would
leave the system in an inconsistent state in which the departement is
associated to an unfinished course, as this transitory state would be
visible to the outside world.

There needs to be a way to persistently store the Write Set of each
step, outside the global context so that the changes are not visible
to the outside world. Only upon committing the Long Lived Transaction
these changes would be merged to the global context, meaning that the
writes performed by each step are effectively delayed until the end of
the transaction. Note that for correctness purposes, the Read Set of
the transaction must also be collected, so that at commit time both
Read Set and Write Set can be replicated, taking advantage of the
already existing JVSTM support.

As the main feature of the Fenix Framework is providing mechanisms to
transactionally manipulate and persist Domain Objects, perhaps it
would be a good aproach to use regular Domain Objects to store the
required information.


\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{tx-context}
\caption{Transactional Context's Domain Model}
\label{fig:transactionalContext}
\end{figure}



\subsection{Data Structures}

Consider the domain model presented in
Figure~\ref{fig:transactionalContext}, as a reification of the data
necessary for a transaction to be successful. A {\bf
  TransactionalContext} is the centrepiece of the Domain, it is a
logical representation of a Long Lived Transaction, holding together
the entire state of the transaction. By being represented in the DML,
the context is kept persistently across the various steps of the
transaction, and is transactionally safe, allowing for multiple
concurrent steps of the Long Lived Transaction.

In this model, the {\bf TransactionalContext} keeps the state of the
transaction (whether it is started, committed, aborted or in
conflict) and the version marker (corresponding to the ``current''
version when the first step of the transaction executed).

A context has two associated sets of {\bf LogEntries}, one for the
Read Set and one for the Write Set. A {\bf LogEntry} represents one
read or written object throughout the transaction, by keeping a
reference to the object, as well as the value that was written (in
case the LogEntry belongs to the Write Set).

Once again recall the Course creation example, in which a Long Lived
Transaction is used to perform a multi-step creation of a new Course
for a given Department. To perform such operation, a new {\bf
  TransactionalContext} is created, initially empty
(Figure~\ref{fig:course-logEntries}a), with an undefined version.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{log-entries}
\caption{Transactional Context instances in a Course creation transaction}
\label{fig:course-logEntries}
\end{figure}

In the first step, a new Course named ``Software Engineering'' is
created, and it is added to the CS department. As this step performed
a single read and two writes, three {\bf LogEntries} are created: 
\begin{inparaenum}[\itshape a\upshape)]
\item In the Read Set representing the reading of the department's
  Course list
\item In the Write Set representing the updaded Course list for the CS
  department
\item In the Write Set representing the name slot for the newly
  created course.
\end{inparaenum}
Note that as this step was the first, the version of the {\bf
  TransactionalContext} is now defined as the ``current''
version. Figure~\ref{fig:course-logEntries}b shows the contents of the
context after the execution of the first step.

\subsection{Behaviour ?!?!}

Having the necessary data structures laid out is crucial for a proper
implementation of Long Lived Transactions, but it is just the
beginning. Whereas these structures are agnostic to the specific
backend, the backend must be able to recognise when a transaction is
executing within the context of a Long Lived Transaction.

In the Fenix Framework, transactions are bound to a specific thread,
allowing for multi-threaded applications to execute multiple
concurrent transactions in different threads. As such, to run a
transaction in the context of a Long Lived Transaction, one must first
bind the context to the current thread. This way, the backend will
check the thread-local variable for a context, to determine whether 
the transaction will be a long transaction step or
not. Listing~\ref{list:longTxBind} shows the programmer API for
binding a {\bf TransactionalContext} to a given thread. This way, the
programmer is free to run any piece of transactional context as a step
of a Long Lived Transaction.

\begin{lstlisting}[caption={Example of TransactionalContext usage},
  label={list:longTxBind}]
public void runStep(TransactionalContext context) {
  try {
    LongTransaction.setContextForThread(context);
    transactionalOperation();
  } finally {
    LongTransaction.removeContextFromThread();
  }
}

@Atomic
public void transactionalOperation() {
  (...)
}
\end{lstlisting}

Transactions occurring within the context of a Long Lived Transactions
must be aware of that fact, as this means that the semantics of domain
getters and setters are changed. 

When reading the value of a slot within a {\bf TransactionalContext},
it is the responsibility of the backend to check whether the slot is
in the write-set of transaction (so that written values can be later
read) and if it isn't, read the value of the slot in the correct
version (the version recorded in the context). Slot reads are
recorded, and stored as a {\bf LogEntry} in the Read Set.

When writing the value of a slot, the written value is stored as a
{\bf LogEntry} in the Write Set, so it can later be retrieved by read
operations and when committing the transaction.

Backends will typically intercept reads and writes to the domain, and
use their regular methods for accessing the underlying transactional
context. As such, in the end of a step, only slots belonging to the
{\bf TransactionalContext} and its {\bf LogEntries} are written to
persistent support.

\subsection{Committing}

So far we have seen what data is stored in a {\bf
  TransactionalContext} and how it is ``populated''. Now we shall look
at what happens when the Long Lived Transaction finishes, and the
context is committed.

Just like in a regular transaction, a Long Lived Transaction must be
atomic and consistent, meaning that its effects must appear to have
occurred at a single well-defined point in time. To accomplish this,
all elements of the Read Set must be validated to be in the same
version, thus ensuring that all writes were performed based on fresh
data (more details on how this is accomplished are given below). If
the validation step is successful, all the written data must be merged
into the global context, by iterating over all {\bf LogEntries} in the
Write Set, and writing the recorded value to the correct slot.

To ensure the correctness of the commit operation, both validation and
merge are performed within a regular transaction, in a
backend-specific manner (as only the backend knows how to write to an
arbitrary slot and to check if the read value is still valid). Any
conflicts on this operation, such as multiple concurrent commit
operations, or writes to validated slots will cause the commit
transaction itself to abort and restart.

Programmers can also manually roll back the Long Lived Transaction. In
this situation, all the information stored in the corresponding {\bf
  TransactionalContext} is deleted. As the Reads and Writes performed
by the transaction are stored exclusively in the context, no further
action is required.

\section{Implementation}
\label{sec:impl}

This section describes the implementation of the architecture proposed
in the previous section. The implementation is divided in two parts:

\begin{itemize}

\item {\bf API} contains the domain specification of the {\bf
    TransactionalContext} and {\bf LogEntries}, as well as the API
  available to the programmer.

\item {\bf JVSTM-based} implementation of Long Lived Transactions.

\end{itemize}

\subsection{API}

The {\bf long-tx-api} module is pretty straightforward. It contains
the domain definition described in Figure~\ref{fig:transactionalContext}, 

The previous section described a solution to ease the development of
Long Lived Transaction. This section describes how that solution was
implemented on the Fenix Framework, using the JVSTM as the
transactional support provider.

The goal of this separation is twofold: to allow alternative
implementations on top of non-JVSTM backends, and to hide internal
implementation from the programmer (who should not depend on backend
code).

\subsection{Programmer API}

In the {\bf long-tx-api} module, programmers can find the available
API to work with Long Lived Transactions.

The first major component of this API is the domain. The domain is
public API, so that Long Lived Transactions can be associated with any
programmer-defined object (e.g., to a user, to a group, a process
etc). This design decision allows for a simple solution, as
cross-cutting concerns such as security and sharing are abstract, and
also gives the programmer more flexibility.

It is the programmer's responsibility to instantiate a new
TransactionalContext every time a new Long Lived Transaction is to be
started. With the context in hand, the programmer simply needs to bind
it to the thread running the step. Listing~\ref{list:longTxBind} shows
the code necessary to bind the context to a thread.

Using only this simple API, the programmer is able to easily code
features that benefit from Long Lived Transactions with little effort.

\subsection{JVSTM implementation}

In the proposed architecture, the backend is required to provide the
following features:

\begin{itemize}

\item {\bf Context Detection} The backend detects when a context is
  present, and begins a specialised transaction.

\item {\bf Intercepting reads/writes} Via the specialised transaction,
  which is aware of the context and delegates to it.

\item {\bf Context commit} Using 

\end{itemize}

\subsubsection{Context Detection}

When beginning a transaction in jvstm-common, the backend checks for
the presence of a {\bf TransactionalContext} bound to the current
thread, and if one is present, the following happens:

\begin{enumerate}

\item A regular transaction is started. This transaction will be used
  to access the context and previous versions of VBoxes (for when a
  read is request and the context does not provide a value).

\item A nested \texttt{LLTStepTransaction} is started. This will be the
  active transaction, and route reads and writes.

\item The version of the context is set as the ``current'' version if
  it wasn't previously set

\end{enumerate}

\begin{lstlisting}[caption={Beginning a new Long Lived Transaction
    step}]

if (LongTransactionSupport.isInsideContext()) {
  TransactionalContext context = 
      LongTransactionSupport.getContextForThread();

  // Begin a new Top Level Write-Transaction
  JvstmInFenixTransaction underlying =
      Transaction.begin(false);

  LLTStepTransaction longTx = 
      new LLTStepTransaction (context, underlying);

  longTx.start();

  transactions.set(new JVSTMTransaction(longTx));
}

(...)

// In the beginning of the new transaction
if (context.getVersion() == null) {
  context.setVersion(Transaction.current().getNumber());
}
\end{lstlisting}

The main reason to use a Nested transaction is to allow portability
across concrete persistence implementations, as the
\texttt{LLTStepTransaction} will be agnostic to the specific
underlying transaction (provided it fulfils the required API). A
\texttt{LLTStepTransaction} holds a reference to the {\bf
  TransactionalContext} backing it, so it can be used to aid in
reading and writing.

\subsubsection{Intercepting Reads and Writes}

\begin{lstlisting}[caption={Reading a VBox within a Context}]
public <T> T getBoxValue(VBox<T> vbox) {
  // Check the local Write Set
  T result = getLocalValue(vbox);
  if (result == null) {
    if (vbox.getOwnerObject().
          getClass().isAnnotationPresent(NoLogEntries.class)) {
      // For classes annotated with @NoLogEntries
      // check the parent transaction in the current version
      return parent.getBoxValue(vbox);
    }

    // Check the TransactionalContext
    result = lookupBoxValueInContext(vbox);
    if (result == null) {
      // Read from the parent
      result = parent.getPreviousBoxValue(vbox, version);
      bodiesRead.put(vbox, null);
    }
  }
  return result == NULL_VALUE ? null : result;
}

private <T> T lookupBoxValueInContext(VBox<T> vbox) {
  LogEntry writeSetEntry = context.getWriteSet();
  while (writeSetEntry != null) {
    if (writeSetEntry.getDomainObject().
                             equals(vbox.getOwnerObject()) &&
        writeSetEntry.getSlotName().
                             equals(vbox.getSlotName())) {
      String contents = writeSetEntry.getContents();
      return vbox.getOwnerObject().
                         getValueFromJSON(vbox.getSlotName(),
                                                       contents);
    }
    writeSetEntry = writeSetEntry.getNextEntry();
  }
  return null;
}


\end{lstlisting}

When reading a VBox within a \texttt{LLTStepTransaction}, the following
steps are executed:

\begin{enumerate}

\item If the VBox was previously written in this step, return the
  written value.

\item If the VBox being read belongs to the Long Lived Transaction
  system (i.e., the domain present in the long-tx-api module),
  delegate the read to the parent in the current version (as these
  classes fall out of control of the context).

\item If the VBox was written in a previous step of the Long Lived
  Transaction, return the previously written value.

\item Else, delegate the read to the underlying transaction, in the
  same version as the context, and add the VBox to the current
  transaction's Read Set.

\end{enumerate}

As domain slots can hold virtually any value, it is not possible to
store the value directly in a single statically typed Fenix Framework
slot. To solve this issue, the values are stored in JSON format, in a
single slot on the {\bf LogEntry} class. Recall from
Section~\ref{sec:json} that the Fenix Framework provides native
support to converting any ValueType to/from JSON. With a little help
from the backend-specific Code Generation step, domain objects can now
be asked to convert the value of a given slot to/from JSON, making it
possible for Long Lived Transactions to get a JSON value from any
VBox.

When the transaction finishes, the \texttt{LLTStepTransaction} has in
its Read and Write Sets the actual VBoxes that were read/written
during the transaction. Recall that in each step the Read Set and
Write Set of the step must contain only the changes to the context
that reflect what has been read and written. As such, when committing
the step, the Read Set and Write Set are processed in the following
manner:

\begin{lstlisting}[caption={Algorithm for committing a Long Lived
    Transaction's step}]
protected void tryCommit() {
  // Go through the ReadSet and the WriteSet
  // No consistency validations are performed here, as the 
  // purpose of this transaction is to manipulate the contents
  // of the read/write set.
  Map<VBox, Object> originalWriteSet = 
                new HashMap<VBox, Object>(boxesWritten);
  Set<VBox> originalReadSet = new HashSet(bodiesRead.keySet());

  // Clear read/write-set, will be populated with LogEntries
  boxesWritten.clear();
  bodiesRead.clear();

  for (Entry<VBox, Object> entry : originalWriteSet.entrySet()) 
  {
    VBox<?> vbox = (VBox<?>) entry.getKey();

    JVSTMDomainObject owner = vbox.getOwnerObject();

    // Get the JSON value with a little help
    // from Generated Code
    String json = 
          owner.getJSONStringForSlot(vbox.getSlotName(), 
                                     entry.getValue());

    // Create or update the LogEntry
    context.addWriteSetEntry(owner, vbox.getSlotName(), json);
  }

  for (VBox vbox : originalReadSet) {
    // Create a new LogEntry if this slot wasn't
    // read in a previous step
    context.addReadSetEntry(vbox.getOwnerObject(), 
                            vbox.getSlotName());
  }

  // Add the new read/written boxes to the parent transaction,
  // which will ensure the LogEntries are properly committed
  super.tryCommit();
}
\end{lstlisting}

\begin{enumerate}

\item The original Write Set is backed up, and the Write Set of the
  nested transaction is cleared.

\item For each item in the original Write Set, the written value is
  converted to JSON and it is added to the {\bf
    TransactionalContext}. As this operation is still running within
  the context of the transaction, the changes are added to the real
  Write Set transparently.

\item The same is done to the Read Set.

\item The new Read and Write Sets are merged with the parent
  transaction, and the nested transaction is committed.

\end{enumerate}

Once the nested transaction is committed, the parent
(backend-specific) transaction will ensure that the updated {\bf
  TransactionalContext} is stored in persistent support.

\subsubsection{Finishing the Long Lived Transaction}

Once all steps of the Long Lived Transaction are finished, the
transaction must be committed, ensuring that the changes performed in
it are visible to the outside world. Much of this process is heavily
dependent on the backend as it involves direct access to the
underlying data structures.

The commit process occurs within a regular transaction, in which all
the data is both read and written, taking advantage of the already
existing transactional system.

The first step in committing the context is verifying whether all the
read data is still valid. As every slot is mapped in a JVSTM VBox, all
the boxes corresponding to the read slots must be verified. The
process iterates over all read slots, locating the VBox that
represents the slot. It then reads the VBox, so that it is added to
the Read Set of the current transaction. Then, the latest version of
the VBox is compared to context's version, thus ensuring that the read
value was the latest. In case the current version is larger than the
read version, the Long Lived Transaction is aborted.

By having the whole Read Set replicated in the committing transaction,
we are ensuring that concurrent transactions writing to the read slots
do not invalidate the version check by updating the version of the
VBox. 

Consider the following scenario: VBox A was read in a Long Lived
Transaction in version 1 and not changed afterwards. When the Long
Lived Transaction commits (in a transaction X), A will be validated,
its current version (1) compared to the version of the transaction
(1). It passes the test and validation succeeds, proceeding with the
commit. Concurrently (after the validation, before X commits), another
transaction writes to A and commits, increasing its version to 2. When
X attempts to commit, its read set (which mirrors the Read Set of the
Long Lived Transaction) is validated. As A was concurrently written, X
will be restarted, and the version verification will fail, marking the
Long Lived Transaction as conflicting.

Once the Read Set of the Long Lived Transaction is validated, the
Write Set must be merged into the global context. This process
iterates over the written slots, and for each slot: a) Locates the
VBox representing it, b) Converts the JSON value to the concrete value
and c) Writes the value to the VBox.


\section{Validation}
\label{sec:validation}


\subsection{Correctness}

Perhaps the greatest challenge on implementing Long Lived Transactions
vs a regular transaction is ensuring that the same correctness
guarantees are provided. This section demonstrates that the presented
solution gives the same transactional guarantees as a regular transaction.

The JVSTM provides all three ACI (Atomicity, Consistency and
Isolation) properties, whereas the Fenix Framework aids in providing
the Durability property (which actually depends on the concrete
backend used). The JVSTM also guarantees an Opacity level of
isolation (refer to Section~\ref{sec:opacity}).

The solution presented in this chapter extends all the ACID properties
to Long Lived Transactions, and in 



\begin{itemize}

\item {\bf Atomicity} is provided as all the writes performed during
  the transaction are only written to the global context when the
  transaction is committed.

\item {\bf Isolation} is provided as all the data is kept in an
  isolated context.

\item {\bf Consistency} is provided in the same way as a regular
  transaction, by ensuring that every read is still valid at commit
  time.

\item {\bf Durability} 

\end{itemize}

Throughout the execution of the Long Lived Transaction, written data
is collected and stored inside the {\bf TransactionalContext}. Once
the transaction is finished, written data is {\bf atomically} written
to the global context, using a regular transaction which simply reads
data from one domain object (the context) to another (the actual
objects written during the transaction).

Long Lived Transactions are {\bf consistent} . Data validity is
guaranteed as the commit algorithm performs a Read Set verification as
if the whole transaction occurred in a single step.

{\bf Durability} is an ``optional'' property, as not all JVSTM-based
backends provide persistent support. Those that do however, ensure
that the effects of Long Lived Transactions as durable, as it works as
if writes occurred in a single regular transaction (which already
provides the Durability property).

{\bf Isolation} is perhaps the hardest property to demonstrate.



\subsection{Ease of Use}

The primary goal of this work is to ease the development of Long Lived
Transactions, and as such, it is rather important to provide a simple
and concise API.

The proposed solution fares well in that regard, as it allows existing
code to be adapted to use Long Lived Transactions with no
modifications! It is possible to program the business logic of your
whole application using regular transactions, and with a simple
wrapper add Long Lived Transaction support.

Consider a Web Application wishing to share with its users the
benefits of Long Lived Transactions, by allowing each individual user
to keep a series of Long Operations. Support for this feature could be
added at an infrastructural level, by providing the user with a UI to
manage his Operations (creating, committing, enabling, etc). Creating
and committing the operation would imply simple domain object
manipulation (in particular creating and committing a {\bf
  TransactionalContext}.

Making every action performed by the user as a step of the Long Lived
Transaction would be a simple as keeping the context in session-local
storage, and binding it before every transaction start. With this
architecture, it is possible to make EVERY operation in the
application part of a Long Lived Transaction, without the need to
change existing code, or changing the methodology used to develop new
functionalities. 
